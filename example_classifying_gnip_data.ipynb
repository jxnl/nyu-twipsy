{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of how you should be using the API\n",
    "\n",
    "First thing we need to do is assume you have a folder of split csv files. Afterwards we'll load classifiers from MongoDB and use them for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __private import fs\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir\n",
    "from classification import dao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we should locate the data and all of the mini-batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_directory = \"./example_data/\"\n",
    "twitter_data = [data_directory+fn for fn in listdir(data_directory)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./example_data/2015_12_29_05_00_activities.csv.gz\n",
      "./example_data/2015_12_29_07_00_activities.csv.gz\n",
      "./example_data/2015_12_29_20_30_activities.csv.gz\n",
      "./example_data/2015_12_29_21_30_activities.csv.gz\n"
     ]
    }
   ],
   "source": [
    "for _ in twitter_data: print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we should look at our available classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alcohol|accuracy:0.8143360752056404|f1:0.8192219679633866|type:LogisticRegression\n",
      "alcohol|accuracy:0.8401880141010576|f1:0.8498896247240618|type:SVC\n",
      "alcohol|accuracy:0.8425381903642774|f1:0.8562231759656651|type:RandomForestClassifier\n",
      "first_person_label|accuracy:0.5637860082304527|f1:0.5574430033343769|type:SVC\n",
      "first_person_label|accuracy:0.5637860082304527|f1:0.5643693591852614|type:LogisticRegression\n",
      "first_person|accuracy:0.6951871657754011|f1:0.8034482758620688|type:RandomForestClassifier\n",
      "first_person|accuracy:0.7005347593582888|f1:0.7751004016064257|type:LogisticRegression\n",
      "first_person|accuracy:0.7032085561497327|f1:0.8062827225130889|type:RandomForestClassifier\n",
      "first_person|accuracy:0.7112299465240641|f1:0.8021978021978021|type:SVC\n"
     ]
    }
   ],
   "source": [
    "for _ in fs.list(): print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the three classifiers we will end up using, primarily for speed and performances\n",
    "\n",
    "```\n",
    "alcohol +- first person +- (present) casual\n",
    "                        +- (future) intention/looking\n",
    "                        +- (past) reflecting\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_fn_alc = \"alcohol|accuracy:0.8143360752056404|f1:0.8192219679633866|type:LogisticRegression\"\n",
    "clf_fn_fpa = \"first_person|accuracy:0.7112299465240641|f1:0.8021978021978021|type:SVC\"\n",
    "clf_fn_fpl = \"first_person_label|accuracy:0.5637860082304527|f1:0.5643693591852614|type:LogisticRegression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.41 s, sys: 2.26 s, total: 4.67 s\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf_alc = dao.ClassifierAccess.get_byfile(clf_fn_alc)\n",
    "clf_fpa = dao.ClassifierAccess.get_byfile(clf_fn_fpa)\n",
    "clf_fpl = dao.ClassifierAccess.get_byfile(clf_fn_fpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How you have all of the classifiers loaded in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': LogisticRegression(C=139.67415702201885, class_weight=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=None, penalty='l2', random_state=None,\n",
       "           solver='liblinear', tol=0.000449897709599141, verbose=0,\n",
       "           warm_start=None),\n",
       " 'clf__C': 139.67415702201885,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__dual': False,\n",
       " 'clf__fit_intercept': True,\n",
       " 'clf__intercept_scaling': 1,\n",
       " 'clf__max_iter': 100,\n",
       " 'clf__multi_class': 'ovr',\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__penalty': 'l2',\n",
       " 'clf__random_state': None,\n",
       " 'clf__solver': 'liblinear',\n",
       " 'clf__tol': 0.000449897709599141,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': None,\n",
       " 'features': FeatureUnion(n_jobs=1,\n",
       "        transformer_list=[('text', Pipeline(steps=[('getter', ItemGetter(key='text')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=False, max_df=1.0, max_features=94963, min_df=1,\n",
       "   ...ry=None))])), ('topic', Pipeline(steps=[('getter', ItemGetter(key='text')), ('topics', Gensim())]))],\n",
       "        transformer_weights=None),\n",
       " 'features__n_jobs': 1,\n",
       " 'features__text': Pipeline(steps=[('getter', ItemGetter(key='text')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=False, max_df=1.0, max_features=94963, min_df=1,\n",
       "         ngram_range=(1, 3), norm='l2',...\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None))]),\n",
       " 'features__text__getter': ItemGetter(key='text'),\n",
       " 'features__text__getter__key': 'text',\n",
       " 'features__text__steps': [('getter', ItemGetter(key='text')),\n",
       "  ('tfidf',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=False, max_df=1.0, max_features=94963, min_df=1,\n",
       "           ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None))],\n",
       " 'features__text__tfidf': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=False, max_df=1.0, max_features=94963, min_df=1,\n",
       "         ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       " 'features__text__tfidf__analyzer': 'word',\n",
       " 'features__text__tfidf__binary': False,\n",
       " 'features__text__tfidf__decode_error': 'strict',\n",
       " 'features__text__tfidf__dtype': numpy.int64,\n",
       " 'features__text__tfidf__encoding': 'utf-8',\n",
       " 'features__text__tfidf__input': 'content',\n",
       " 'features__text__tfidf__lowercase': False,\n",
       " 'features__text__tfidf__max_df': 1.0,\n",
       " 'features__text__tfidf__max_features': 94963,\n",
       " 'features__text__tfidf__min_df': 1,\n",
       " 'features__text__tfidf__ngram_range': (1, 3),\n",
       " 'features__text__tfidf__norm': 'l2',\n",
       " 'features__text__tfidf__preprocessor': None,\n",
       " 'features__text__tfidf__smooth_idf': True,\n",
       " 'features__text__tfidf__stop_words': None,\n",
       " 'features__text__tfidf__strip_accents': None,\n",
       " 'features__text__tfidf__sublinear_tf': False,\n",
       " 'features__text__tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'features__text__tfidf__tokenizer': None,\n",
       " 'features__text__tfidf__use_idf': True,\n",
       " 'features__text__tfidf__vocabulary': None,\n",
       " 'features__topic': Pipeline(steps=[('getter', ItemGetter(key='text')), ('topics', Gensim())]),\n",
       " 'features__topic__getter': ItemGetter(key='text'),\n",
       " 'features__topic__getter__key': 'text',\n",
       " 'features__topic__steps': [('getter', ItemGetter(key='text')),\n",
       "  ('topics', Gensim())],\n",
       " 'features__topic__topics': Gensim(),\n",
       " 'features__transformer_list': [('text',\n",
       "   Pipeline(steps=[('getter', ItemGetter(key='text')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=False, max_df=1.0, max_features=94963, min_df=1,\n",
       "           ngram_range=(1, 3), norm='l2',...\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None))])),\n",
       "  ('topic',\n",
       "   Pipeline(steps=[('getter', ItemGetter(key='text')), ('topics', Gensim())]))],\n",
       " 'features__transformer_weights': None,\n",
       " 'scaler': Normalizer(copy=True, norm='l2'),\n",
       " 'scaler__copy': True,\n",
       " 'scaler__norm': 'l2',\n",
       " 'steps': [('features', FeatureUnion(n_jobs=1,\n",
       "          transformer_list=[('text', Pipeline(steps=[('getter', ItemGetter(key='text')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=False, max_df=1.0, max_features=94963, min_df=1,\n",
       "     ...ry=None))])), ('topic', Pipeline(steps=[('getter', ItemGetter(key='text')), ('topics', Gensim())]))],\n",
       "          transformer_weights=None)),\n",
       "  ('scaler', Normalizer(copy=True, norm='l2')),\n",
       "  ('clf',\n",
       "   LogisticRegression(C=139.67415702201885, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=None, penalty='l2', random_state=None,\n",
       "             solver='liblinear', tol=0.000449897709599141, verbose=0,\n",
       "             warm_start=None))]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_fpl.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models however may not be trained up with most recent data, this is why we want to then load in all the training data and re train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.3 s, sys: 1.57 s, total: 3.87 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from data import DataAccess, LabelGetter\n",
    "\n",
    "X = DataAccess.get_as_dataframe()\n",
    "L = LabelGetter(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also note that we are not going to refit the model for alcohol since we believe its performing good enought. the first person model also takes a whole 10minutes so beware eh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.29 s, sys: 3.24 s, total: 12.5 s\n",
      "Wall time: 17.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('text', Pipeline(steps=[('getter', ItemGetter(key='text')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=F...one,\n",
       "          solver='liblinear', tol=0.000449897709599141, verbose=0,\n",
       "          warm_start=None))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf_fpl.fit(*L.get_first_person_label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 54s, sys: 8.71 s, total: 9min 3s\n",
      "Wall time: 9min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('text', Pipeline(steps=[('getter', ItemGetter(key='text')), ('tfidf', TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=F...probability=True,\n",
       "  random_state=None, shrinking=True, tol=0.0008753898561476732,\n",
       "  verbose=False))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf_fpa.fit(*L.get_first_person())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note: you should maybe save these retraiend models onto disk to save you time in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading some DataFrames to classify\n",
    "\n",
    "Note that that there is a column for `text` thats all we care about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5759\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(twitter_data[1])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>first_name</th>\n",
       "      <th>id</th>\n",
       "      <th>place</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Shawn</td>\n",
       "      <td>681731561901830144</td>\n",
       "      <td>Joint Base Lewis-McChord, WA</td>\n",
       "      <td>...listening to Stick Talk for the first time....</td>\n",
       "      <td>2015-12-29T07:00:44.000Z</td>\n",
       "      <td>id:twitter.com:99682561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1Eyewitness</td>\n",
       "      <td>681731560232468480</td>\n",
       "      <td>Burbank, CA</td>\n",
       "      <td>@debrcarter @3ChicsPolitico @LorettaLynch but ...</td>\n",
       "      <td>2015-12-29T07:00:44.000Z</td>\n",
       "      <td>id:twitter.com:1517261017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Evette</td>\n",
       "      <td>681731564338806785</td>\n",
       "      <td>Nashville, TN</td>\n",
       "      <td>middle part üíÅüèæ https://t.co/4Cuzip3KJ6</td>\n",
       "      <td>2015-12-29T07:00:45.000Z</td>\n",
       "      <td>id:twitter.com:2710958186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Guapaholics‚ú®</td>\n",
       "      <td>681731561344122880</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>@frankieclermont @Luisfinessee @desix3__ u kno...</td>\n",
       "      <td>2015-12-29T07:00:44.000Z</td>\n",
       "      <td>id:twitter.com:1190480407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Im</td>\n",
       "      <td>681731564816879616</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>If I give you a nick name I'm the only person ...</td>\n",
       "      <td>2015-12-29T07:00:45.000Z</td>\n",
       "      <td>id:twitter.com:2227423789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    first_name                  id                         place  \\\n",
       "0           0         Shawn  681731561901830144  Joint Base Lewis-McChord, WA   \n",
       "1           1   1Eyewitness  681731560232468480                   Burbank, CA   \n",
       "2           2        Evette  681731564338806785                 Nashville, TN   \n",
       "3           3  Guapaholics‚ú®  681731561344122880                    Boston, MA   \n",
       "4           4            Im  681731564816879616                   Houston, TX   \n",
       "\n",
       "                                                text  \\\n",
       "0  ...listening to Stick Talk for the first time....   \n",
       "1  @debrcarter @3ChicsPolitico @LorettaLynch but ...   \n",
       "2             middle part üíÅüèæ https://t.co/4Cuzip3KJ6   \n",
       "3  @frankieclermont @Luisfinessee @desix3__ u kno...   \n",
       "4  If I give you a nick name I'm the only person ...   \n",
       "\n",
       "                       time                    user_id  \n",
       "0  2015-12-29T07:00:44.000Z    id:twitter.com:99682561  \n",
       "1  2015-12-29T07:00:44.000Z  id:twitter.com:1517261017  \n",
       "2  2015-12-29T07:00:45.000Z  id:twitter.com:2710958186  \n",
       "3  2015-12-29T07:00:44.000Z  id:twitter.com:1190480407  \n",
       "4  2015-12-29T07:00:45.000Z  id:twitter.com:2227423789  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_alc = clf_alc.predict_proba(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5759, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.95334503,  0.04665497],\n",
       "       [ 0.97256387,  0.02743613],\n",
       "       [ 0.79522246,  0.20477754],\n",
       "       ..., \n",
       "       [ 0.97305573,  0.02694427],\n",
       "       [ 0.96458816,  0.03541184],\n",
       "       [ 0.95874137,  0.04125863]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions_alc.shape)\n",
    "predictions_alc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that predictions is a (n, 2) since we are asking for probabilities.\n",
    "we only need one dimension in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"predict_alc\"] = predictions_alc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>first_name</th>\n",
       "      <th>id</th>\n",
       "      <th>place</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>predict_alc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Shawn</td>\n",
       "      <td>681731561901830144</td>\n",
       "      <td>Joint Base Lewis-McChord, WA</td>\n",
       "      <td>...listening to Stick Talk for the first time....</td>\n",
       "      <td>2015-12-29T07:00:44.000Z</td>\n",
       "      <td>id:twitter.com:99682561</td>\n",
       "      <td>0.046655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1Eyewitness</td>\n",
       "      <td>681731560232468480</td>\n",
       "      <td>Burbank, CA</td>\n",
       "      <td>@debrcarter @3ChicsPolitico @LorettaLynch but ...</td>\n",
       "      <td>2015-12-29T07:00:44.000Z</td>\n",
       "      <td>id:twitter.com:1517261017</td>\n",
       "      <td>0.027436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Evette</td>\n",
       "      <td>681731564338806785</td>\n",
       "      <td>Nashville, TN</td>\n",
       "      <td>middle part üíÅüèæ https://t.co/4Cuzip3KJ6</td>\n",
       "      <td>2015-12-29T07:00:45.000Z</td>\n",
       "      <td>id:twitter.com:2710958186</td>\n",
       "      <td>0.204778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Guapaholics‚ú®</td>\n",
       "      <td>681731561344122880</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>@frankieclermont @Luisfinessee @desix3__ u kno...</td>\n",
       "      <td>2015-12-29T07:00:44.000Z</td>\n",
       "      <td>id:twitter.com:1190480407</td>\n",
       "      <td>0.046854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Im</td>\n",
       "      <td>681731564816879616</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>If I give you a nick name I'm the only person ...</td>\n",
       "      <td>2015-12-29T07:00:45.000Z</td>\n",
       "      <td>id:twitter.com:2227423789</td>\n",
       "      <td>0.507233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    first_name                  id                         place  \\\n",
       "0           0         Shawn  681731561901830144  Joint Base Lewis-McChord, WA   \n",
       "1           1   1Eyewitness  681731560232468480                   Burbank, CA   \n",
       "2           2        Evette  681731564338806785                 Nashville, TN   \n",
       "3           3  Guapaholics‚ú®  681731561344122880                    Boston, MA   \n",
       "4           4            Im  681731564816879616                   Houston, TX   \n",
       "\n",
       "                                                text  \\\n",
       "0  ...listening to Stick Talk for the first time....   \n",
       "1  @debrcarter @3ChicsPolitico @LorettaLynch but ...   \n",
       "2             middle part üíÅüèæ https://t.co/4Cuzip3KJ6   \n",
       "3  @frankieclermont @Luisfinessee @desix3__ u kno...   \n",
       "4  If I give you a nick name I'm the only person ...   \n",
       "\n",
       "                       time                    user_id  predict_alc  \n",
       "0  2015-12-29T07:00:44.000Z    id:twitter.com:99682561     0.046655  \n",
       "1  2015-12-29T07:00:44.000Z  id:twitter.com:1517261017     0.027436  \n",
       "2  2015-12-29T07:00:45.000Z  id:twitter.com:2710958186     0.204778  \n",
       "3  2015-12-29T07:00:44.000Z  id:twitter.com:1190480407     0.046854  \n",
       "4  2015-12-29T07:00:45.000Z  id:twitter.com:2227423789     0.507233  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we make the probability predictions for any alcohol related topics we then want to predict the conditional then multiple by the marginal to get the marginal probability of being first person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09480812641083522"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thres = 0.75\n",
    "filter_alc = df.predict_alc > thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"predict_fpa|alc\"] = 0 \n",
    "\n",
    "predict_fpa = clf_fpa.predict_proba(df[filter_alc])\n",
    "df.loc[filter_alc, \"predict_fpa|alc\"] = predict_fpa[:,1]\n",
    "\n",
    "df[\"predict_fpa\"] = df[\"predict_alc\"] * df[\"predict_fpa|alc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will predict again on the levels of being first person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_fpl = clf_fpl.predict_proba(df[filter_alc])\n",
    "\n",
    "predict_fpl = pd.DataFrame(\n",
    "    predict_fpl, \n",
    "         columns=[\n",
    "        \"predict_present|fpa\", \n",
    "        \"predict_future|fpa\", \n",
    "        \"predict_past|fpa\"],\n",
    "    index=df[filter_alc].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in predict_fpl.columns:\n",
    "    predict_fpl[col.split(\"|\")[0]] = predict_fpl[col] * df[filter_alc][\"predict_fpa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.join(predict_fpl).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>first_name</th>\n",
       "      <th>id</th>\n",
       "      <th>place</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>predict_alc</th>\n",
       "      <th>predict_fpa</th>\n",
       "      <th>predict_fpa|alc</th>\n",
       "      <th>predict_present|fpa</th>\n",
       "      <th>predict_future|fpa</th>\n",
       "      <th>predict_past|fpa</th>\n",
       "      <th>predict_present</th>\n",
       "      <th>predict_future</th>\n",
       "      <th>predict_past</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>681731566649737218</td>\n",
       "      <td>Rancho Cucamonga, CA</td>\n",
       "      <td>@achilles314 idk about that man lol thanks though</td>\n",
       "      <td>2015-12-29T07:00:46.000Z</td>\n",
       "      <td>id:twitter.com:2272481959</td>\n",
       "      <td>0.845886</td>\n",
       "      <td>0.446556</td>\n",
       "      <td>0.527915</td>\n",
       "      <td>0.246851</td>\n",
       "      <td>0.533771</td>\n",
       "      <td>0.219378</td>\n",
       "      <td>0.110233</td>\n",
       "      <td>0.238359</td>\n",
       "      <td>0.097965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>Alyssa</td>\n",
       "      <td>681731581053153282</td>\n",
       "      <td>Harmar, PA</td>\n",
       "      <td>My party at work tonight tipped me a wad of $2...</td>\n",
       "      <td>2015-12-29T07:00:49.000Z</td>\n",
       "      <td>id:twitter.com:366101161</td>\n",
       "      <td>0.839278</td>\n",
       "      <td>0.652919</td>\n",
       "      <td>0.777953</td>\n",
       "      <td>0.033023</td>\n",
       "      <td>0.939057</td>\n",
       "      <td>0.027920</td>\n",
       "      <td>0.021562</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.018229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>Kaylarae</td>\n",
       "      <td>681731586476216320</td>\n",
       "      <td>Santa Clarita, CA</td>\n",
       "      <td>Frantically cleans room because I don't feel v...</td>\n",
       "      <td>2015-12-29T07:00:50.000Z</td>\n",
       "      <td>id:twitter.com:1113362977</td>\n",
       "      <td>0.786318</td>\n",
       "      <td>0.331858</td>\n",
       "      <td>0.422041</td>\n",
       "      <td>0.026238</td>\n",
       "      <td>0.003065</td>\n",
       "      <td>0.970697</td>\n",
       "      <td>0.008707</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.322134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>B</td>\n",
       "      <td>681731588502192128</td>\n",
       "      <td>Brownsburg, IN</td>\n",
       "      <td>girls only say I hate you to the guys that the...</td>\n",
       "      <td>2015-12-29T07:00:51.000Z</td>\n",
       "      <td>id:twitter.com:1158223614</td>\n",
       "      <td>0.756622</td>\n",
       "      <td>0.149693</td>\n",
       "      <td>0.197844</td>\n",
       "      <td>0.058818</td>\n",
       "      <td>0.009190</td>\n",
       "      <td>0.931992</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.139512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>‚Ä¢Kristina</td>\n",
       "      <td>681731590343409664</td>\n",
       "      <td>Nuevo, CA</td>\n",
       "      <td>Your babeüòç</td>\n",
       "      <td>2015-12-29T07:00:51.000Z</td>\n",
       "      <td>id:twitter.com:4502766632</td>\n",
       "      <td>0.900311</td>\n",
       "      <td>0.240806</td>\n",
       "      <td>0.267470</td>\n",
       "      <td>0.512077</td>\n",
       "      <td>0.069297</td>\n",
       "      <td>0.418625</td>\n",
       "      <td>0.123311</td>\n",
       "      <td>0.016687</td>\n",
       "      <td>0.100808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0 first_name                  id                 place  \\\n",
       "9            9       Ryan  681731566649737218  Rancho Cucamonga, CA   \n",
       "47          47     Alyssa  681731581053153282            Harmar, PA   \n",
       "51          51   Kaylarae  681731586476216320     Santa Clarita, CA   \n",
       "59          59          B  681731588502192128        Brownsburg, IN   \n",
       "66          66  ‚Ä¢Kristina  681731590343409664             Nuevo, CA   \n",
       "\n",
       "                                                 text  \\\n",
       "9   @achilles314 idk about that man lol thanks though   \n",
       "47  My party at work tonight tipped me a wad of $2...   \n",
       "51  Frantically cleans room because I don't feel v...   \n",
       "59  girls only say I hate you to the guys that the...   \n",
       "66                                         Your babeüòç   \n",
       "\n",
       "                        time                    user_id  predict_alc  \\\n",
       "9   2015-12-29T07:00:46.000Z  id:twitter.com:2272481959     0.845886   \n",
       "47  2015-12-29T07:00:49.000Z   id:twitter.com:366101161     0.839278   \n",
       "51  2015-12-29T07:00:50.000Z  id:twitter.com:1113362977     0.786318   \n",
       "59  2015-12-29T07:00:51.000Z  id:twitter.com:1158223614     0.756622   \n",
       "66  2015-12-29T07:00:51.000Z  id:twitter.com:4502766632     0.900311   \n",
       "\n",
       "    predict_fpa  predict_fpa|alc  predict_present|fpa  predict_future|fpa  \\\n",
       "9      0.446556         0.527915             0.246851            0.533771   \n",
       "47     0.652919         0.777953             0.033023            0.939057   \n",
       "51     0.331858         0.422041             0.026238            0.003065   \n",
       "59     0.149693         0.197844             0.058818            0.009190   \n",
       "66     0.240806         0.267470             0.512077            0.069297   \n",
       "\n",
       "    predict_past|fpa  predict_present  predict_future  predict_past  \n",
       "9           0.219378         0.110233        0.238359      0.097965  \n",
       "47          0.027920         0.021562        0.613128      0.018229  \n",
       "51          0.970697         0.008707        0.001017      0.322134  \n",
       "59          0.931992         0.008805        0.001376      0.139512  \n",
       "66          0.418625         0.123311        0.016687      0.100808  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.predict_alc > thres].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'first_name', 'id', 'place', 'text', 'time', 'user_id',\n",
       "       'predict_alc', 'predict_fpa', 'predict_fpa|alc', 'predict_present|fpa',\n",
       "       'predict_future|fpa', 'predict_past|fpa', 'predict_present',\n",
       "       'predict_future', 'predict_past'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giving you a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PredictionTransformer:\n",
    "    \n",
    "    cols = [\n",
    "        'predict_alc', \n",
    "        'predict_fpa', \n",
    "        'predict_fpa|alc',\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, clf_alc, clf_fpa, clf_fpl):\n",
    "        self.clf_alc = clf_alc\n",
    "        self.clf_fpa = clf_fpa\n",
    "        self.clf_fpl = clf_fpl\n",
    "        \n",
    "    def __call__(self, df, thres=0.75):\n",
    "        self.df = df\n",
    "        \n",
    "        for col in self.cols:\n",
    "            self.df[col] = 0 \n",
    "        \n",
    "        self.thres = thres\n",
    "        \n",
    "        self._make_alcohol_predictions()\n",
    "        self._make_firstperson_predictions()\n",
    "        self._make_firstpersonlevel_predictions()\n",
    "        \n",
    "        return self.df\n",
    "        \n",
    "    \n",
    "    def _make_alcohol_predictions(self):\n",
    "        predictions_alc = self.clf_alc.predict_proba(self.df)\n",
    "        self.df[\"predict_alc\"] = predictions_alc[:,1]\n",
    "    \n",
    "    def _make_firstperson_predictions(self):\n",
    "        filter_alc = self.df.predict_alc > self.thres\n",
    "\n",
    "        # predict only on subset of the data, makes things way faster\n",
    "        predict_fpa = self.clf_fpa.predict_proba(self.df[filter_alc])\n",
    "        self.df.loc[filter_alc, \"predict_fpa|alc\"] = predict_fpa[:,1]\n",
    "\n",
    "        # compute a marginal using the product rule\n",
    "        self.df[\"predict_fpa\"] = self.df[\"predict_alc\"] * self.df[\"predict_fpa|alc\"]\n",
    "        \n",
    "    def _make_firstpersonlevel_predictions(self):\n",
    "        filter_alc = self.df.predict_alc > self.thres\n",
    "        \n",
    "        # predict only on subset of the data, makes things way faster\n",
    "        predict_fpl = self.clf_fpl.predict_proba(self.df[filter_alc])\n",
    "\n",
    "        # convert it to a named dataframe\n",
    "        predict_fpl = pd.DataFrame(\n",
    "            predict_fpl, \n",
    "                 columns=[\n",
    "                \"predict_present|fpa\", \n",
    "                \"predict_future|fpa\", \n",
    "                \"predict_past|fpa\"],\n",
    "            index=self.df[filter_alc].index)\n",
    "        \n",
    "        marginal_firstperson = self.df[filter_alc][\"predict_fpa\"]\n",
    "        \n",
    "        # for each conditional level generate a marginal\n",
    "        for col in predict_fpl.columns:\n",
    "            col_marginal = col.split(\"|\")[0]\n",
    "            predict_fpl[col_marginal] = predict_fpl[col] * marginal_firstperson\n",
    "            \n",
    "        self.df = self.df.join(predict_fpl).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = PredictionTransformer(clf_alc, clf_fpa, clf_fpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeld_dataframe = clf(pd.read_csv(twitter_data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict_fpa</th>\n",
       "      <th>predict_alc</th>\n",
       "      <th>predict_present</th>\n",
       "      <th>predict_future</th>\n",
       "      <th>predict_past</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.708982</td>\n",
       "      <td>0.911998</td>\n",
       "      <td>0.130708</td>\n",
       "      <td>0.568470</td>\n",
       "      <td>0.009805</td>\n",
       "      <td>I can't wait for the day to be cuddled up on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3543</th>\n",
       "      <td>0.656804</td>\n",
       "      <td>0.892279</td>\n",
       "      <td>0.639164</td>\n",
       "      <td>0.011552</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>@msremmos Nah, Im still happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.784311</td>\n",
       "      <td>0.859950</td>\n",
       "      <td>0.099506</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.684518</td>\n",
       "      <td>I think I've gained more weight being at home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>0.708940</td>\n",
       "      <td>0.978766</td>\n",
       "      <td>0.488762</td>\n",
       "      <td>0.207091</td>\n",
       "      <td>0.013087</td>\n",
       "      <td>French Toast &amp;gt;&amp;gt;&amp;gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.703618</td>\n",
       "      <td>0.994573</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.233740</td>\n",
       "      <td>0.466030</td>\n",
       "      <td>Last semester of hs is coming up and I have ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>0.754629</td>\n",
       "      <td>0.886594</td>\n",
       "      <td>0.747106</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.007509</td>\n",
       "      <td>Thanks, swayz-dog!! Never had a #gbot before #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4087</th>\n",
       "      <td>0.628641</td>\n",
       "      <td>0.972433</td>\n",
       "      <td>0.627732</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>Drinking a Red Ale by @MarbleBrewery at @marbl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323</th>\n",
       "      <td>0.749898</td>\n",
       "      <td>0.981646</td>\n",
       "      <td>0.718458</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.027298</td>\n",
       "      <td>Pinky toe üòêüò©üò©üò©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>0.754613</td>\n",
       "      <td>0.989356</td>\n",
       "      <td>0.216126</td>\n",
       "      <td>0.529834</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>ive been tryna go home n play xbox allll day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4326</th>\n",
       "      <td>0.715525</td>\n",
       "      <td>0.924749</td>\n",
       "      <td>0.475757</td>\n",
       "      <td>0.087332</td>\n",
       "      <td>0.152437</td>\n",
       "      <td>it's lowkey at the night show</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      predict_fpa  predict_alc  predict_present  predict_future  predict_past  \\\n",
       "98       0.708982     0.911998         0.130708        0.568470      0.009805   \n",
       "3543     0.656804     0.892279         0.639164        0.011552      0.006089   \n",
       "149      0.784311     0.859950         0.099506        0.000288      0.684518   \n",
       "1938     0.708940     0.978766         0.488762        0.207091      0.013087   \n",
       "214      0.703618     0.994573         0.003848        0.233740      0.466030   \n",
       "1189     0.754629     0.886594         0.747106        0.000014      0.007509   \n",
       "4087     0.628641     0.972433         0.627732        0.000429      0.000481   \n",
       "3323     0.749898     0.981646         0.718458        0.004142      0.027298   \n",
       "5100     0.754613     0.989356         0.216126        0.529834      0.008653   \n",
       "4326     0.715525     0.924749         0.475757        0.087332      0.152437   \n",
       "\n",
       "                                                   text  \n",
       "98    I can't wait for the day to be cuddled up on t...  \n",
       "3543                      @msremmos Nah, Im still happy  \n",
       "149   I think I've gained more weight being at home ...  \n",
       "1938                          French Toast &gt;&gt;&gt;  \n",
       "214   Last semester of hs is coming up and I have ne...  \n",
       "1189  Thanks, swayz-dog!! Never had a #gbot before #...  \n",
       "4087  Drinking a Red Ale by @MarbleBrewery at @marbl...  \n",
       "3323                                     Pinky toe üòêüò©üò©üò©  \n",
       "5100       ive been tryna go home n play xbox allll day  \n",
       "4326                      it's lowkey at the night show  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeld_dataframe[\n",
    "    [\"predict_fpa\", \"predict_alc\", \"predict_present\", \"predict_future\", \"predict_past\", \"text\"]\n",
    "][(labeld_dataframe.predict_fpa\t > .70) \n",
    "  | (labeld_dataframe.predict_present > .60)\n",
    "  | (labeld_dataframe.predict_past > .60)\n",
    "  | (labeld_dataframe.predict_future > .60)\n",
    " ].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can tell the classifications are not amazing for this small dataset. however this is because i'm only taking some a 10minute slice of the data. I'll be better in practise since this is only 60 tweets out of millions and it will in fact catch more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
